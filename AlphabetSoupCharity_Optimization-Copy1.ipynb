{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverable 1: Preprocessing the Data for a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EIN</th>\n",
       "      <th>NAME</th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10520599</td>\n",
       "      <td>BLUE KNIGHTS MOTORCYCLE CLUB</td>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10531628</td>\n",
       "      <td>AMERICAN CHESAPEAKE CLUB CHARITABLE TR</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10547893</td>\n",
       "      <td>ST CLOUD PROFESSIONAL FIREFIGHTERS</td>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10553066</td>\n",
       "      <td>SOUTHSIDE ATHLETIC ASSOCIATION</td>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10556103</td>\n",
       "      <td>GENETIC RESEARCH INSTITUTE OF THE DESERT</td>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        EIN                                      NAME APPLICATION_TYPE  \\\n",
       "0  10520599              BLUE KNIGHTS MOTORCYCLE CLUB              T10   \n",
       "1  10531628    AMERICAN CHESAPEAKE CLUB CHARITABLE TR               T3   \n",
       "2  10547893        ST CLOUD PROFESSIONAL FIREFIGHTERS               T5   \n",
       "3  10553066            SOUTHSIDE ATHLETIC ASSOCIATION               T3   \n",
       "4  10556103  GENETIC RESEARCH INSTITUTE OF THE DESERT               T3   \n",
       "\n",
       "        AFFILIATION CLASSIFICATION      USE_CASE  ORGANIZATION  STATUS  \\\n",
       "0       Independent          C1000    ProductDev   Association       1   \n",
       "1       Independent          C2000  Preservation  Co-operative       1   \n",
       "2  CompanySponsored          C3000    ProductDev   Association       1   \n",
       "3  CompanySponsored          C2000  Preservation         Trust       1   \n",
       "4       Independent          C1000     Heathcare         Trust       1   \n",
       "\n",
       "      INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  IS_SUCCESSFUL  \n",
       "0              0                      N     5000              1  \n",
       "1         1-9999                      N   108590              1  \n",
       "2              0                      N     5000              0  \n",
       "3    10000-24999                      N     6692              1  \n",
       "4  100000-499999                      N   142590              1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "#  Import and read the charity_data.csv.\n",
    "import pandas as pd \n",
    "application_df = pd.read_csv(\"../Resources/charity_data.csv\")\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any null values and duplicates\n",
    "application_df.dropna(inplace=True)\n",
    "application_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE</th>\n",
       "      <th>AFFILIATION</th>\n",
       "      <th>CLASSIFICATION</th>\n",
       "      <th>USE_CASE</th>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <th>STATUS</th>\n",
       "      <th>INCOME_AMT</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T10</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Co-operative</td>\n",
       "      <td>1</td>\n",
       "      <td>1-9999</td>\n",
       "      <td>N</td>\n",
       "      <td>108590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C3000</td>\n",
       "      <td>ProductDev</td>\n",
       "      <td>Association</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T3</td>\n",
       "      <td>CompanySponsored</td>\n",
       "      <td>C2000</td>\n",
       "      <td>Preservation</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>10000-24999</td>\n",
       "      <td>N</td>\n",
       "      <td>6692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T3</td>\n",
       "      <td>Independent</td>\n",
       "      <td>C1000</td>\n",
       "      <td>Heathcare</td>\n",
       "      <td>Trust</td>\n",
       "      <td>1</td>\n",
       "      <td>100000-499999</td>\n",
       "      <td>N</td>\n",
       "      <td>142590</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  APPLICATION_TYPE       AFFILIATION CLASSIFICATION      USE_CASE  \\\n",
       "0              T10       Independent          C1000    ProductDev   \n",
       "1               T3       Independent          C2000  Preservation   \n",
       "2               T5  CompanySponsored          C3000    ProductDev   \n",
       "3               T3  CompanySponsored          C2000  Preservation   \n",
       "4               T3       Independent          C1000     Heathcare   \n",
       "\n",
       "   ORGANIZATION  STATUS     INCOME_AMT SPECIAL_CONSIDERATIONS  ASK_AMT  \\\n",
       "0   Association       1              0                      N     5000   \n",
       "1  Co-operative       1         1-9999                      N   108590   \n",
       "2   Association       1              0                      N     5000   \n",
       "3         Trust       1    10000-24999                      N     6692   \n",
       "4         Trust       1  100000-499999                      N   142590   \n",
       "\n",
       "   IS_SUCCESSFUL  \n",
       "0              1  \n",
       "1              1  \n",
       "2              0  \n",
       "3              1  \n",
       "4              1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n",
    "application_df.drop(columns=[\"EIN\",\"NAME\"],inplace=True)\n",
    "application_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APPLICATION_TYPE            17\n",
       "AFFILIATION                  6\n",
       "CLASSIFICATION              71\n",
       "USE_CASE                     5\n",
       "ORGANIZATION                 4\n",
       "STATUS                       2\n",
       "INCOME_AMT                   9\n",
       "SPECIAL_CONSIDERATIONS       2\n",
       "ASK_AMT                   8747\n",
       "IS_SUCCESSFUL                2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "application_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3     27037\n",
       "T4      1542\n",
       "T6      1216\n",
       "T5      1173\n",
       "T19     1065\n",
       "T8       737\n",
       "T7       725\n",
       "T10      528\n",
       "T9       156\n",
       "T13       66\n",
       "T12       27\n",
       "T2        16\n",
       "T25        3\n",
       "T14        3\n",
       "T29        2\n",
       "T15        2\n",
       "T17        1\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at APPLICATION_TYPE value counts for binning\n",
    "app_type_counts = application_df.APPLICATION_TYPE.value_counts()\n",
    "app_type_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: ylabel='Density'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the value counts of APPLICATION_TYPE\n",
    "app_type_counts.plot.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T3       27037\n",
       "T4        1542\n",
       "T6        1216\n",
       "T5        1173\n",
       "T19       1065\n",
       "T8         737\n",
       "T7         725\n",
       "T10        528\n",
       "Other      276\n",
       "Name: APPLICATION_TYPE, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine which values to replace if counts are less than ...?\n",
    "replace_application = list(app_type_counts[app_type_counts<500].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for app in replace_application:\n",
    "    application_df.APPLICATION_TYPE = application_df.APPLICATION_TYPE.replace(app,\"Other\")\n",
    "    \n",
    "# Check to make sure binning was successful\n",
    "application_df.APPLICATION_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "C7000      777\n",
       "C1700      287\n",
       "C4000      194\n",
       "C5000      116\n",
       "C1270      114\n",
       "C2700      104\n",
       "C2800       95\n",
       "C7100       75\n",
       "C1300       58\n",
       "C1280       50\n",
       "C1230       36\n",
       "C1400       34\n",
       "C7200       32\n",
       "C2300       32\n",
       "C1240       30\n",
       "C8000       20\n",
       "C7120       18\n",
       "C1500       16\n",
       "C1800       15\n",
       "C6000       15\n",
       "C1250       14\n",
       "C8200       11\n",
       "C1238       10\n",
       "C1278       10\n",
       "C1235        9\n",
       "C1237        9\n",
       "C7210        7\n",
       "C2400        6\n",
       "C1720        6\n",
       "C4100        6\n",
       "C1257        5\n",
       "C1600        5\n",
       "C1260        3\n",
       "C2710        3\n",
       "C0           3\n",
       "C3200        2\n",
       "C1234        2\n",
       "C1246        2\n",
       "C1267        2\n",
       "C1256        2\n",
       "C2190        1\n",
       "C4200        1\n",
       "C2600        1\n",
       "C5200        1\n",
       "C1370        1\n",
       "Name: CLASSIFICATION, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at CLASSIFICATION value counts for binning\n",
    "class_counts = application_df.CLASSIFICATION.value_counts()\n",
    "class_counts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: ylabel='Density'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the value counts of CLASSIFICATION\n",
    "class_counts.plot.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "C1000    17326\n",
       "C2000     6074\n",
       "C1200     4837\n",
       "Other     2261\n",
       "C3000     1918\n",
       "C2100     1883\n",
       "Name: CLASSIFICATION, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine which values to replace if counts are less than ..?\n",
    "replace_class = list(class_counts[class_counts<1880].index)\n",
    "\n",
    "# Replace in dataframe\n",
    "for cls in replace_class:\n",
    "    application_df.CLASSIFICATION = application_df.CLASSIFICATION.replace(cls,\"Other\")\n",
    "    \n",
    "# Check to make sure binning was successful\n",
    "new_class_counts = application_df.CLASSIFICATION.value_counts()\n",
    "new_class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APPLICATION_TYPE             9\n",
       "AFFILIATION                  6\n",
       "CLASSIFICATION               6\n",
       "USE_CASE                     5\n",
       "ORGANIZATION                 4\n",
       "STATUS                       2\n",
       "INCOME_AMT                   9\n",
       "SPECIAL_CONSIDERATIONS       2\n",
       "ASK_AMT                   8747\n",
       "IS_SUCCESSFUL                2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of unique values in each column.\n",
    "application_df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Look at INCOME_AMT value counts for binning\n",
    "# income_counts = application_df.INCOME_AMT.value_counts()\n",
    "# income_counts.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the value counts of INCOME_AMT\n",
    "# income_counts.plot.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine which values to replace if counts are less than ..?\n",
    "# # inc_list = ['10M-50M','5M-10M','50M+']\n",
    "# replace_income = list(income_counts[income_counts<500].index)\n",
    "\n",
    "# # Replace in dataframe\n",
    "# for inc in replace_income:\n",
    "#     application_df.INCOME_AMT = application_df.INCOME_AMT.replace(inc,\"Other\")\n",
    "    \n",
    "# # Check to make sure binning was successful\n",
    "# new_income_counts = application_df.INCOME_AMT.value_counts()\n",
    "# new_income_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['APPLICATION_TYPE',\n",
       " 'AFFILIATION',\n",
       " 'CLASSIFICATION',\n",
       " 'USE_CASE',\n",
       " 'ORGANIZATION',\n",
       " 'INCOME_AMT',\n",
       " 'SPECIAL_CONSIDERATIONS']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate our categorical variable lists\n",
    "application_cat = application_df.dtypes[application_df.dtypes == \"object\"].index.tolist()\n",
    "application_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "APPLICATION_TYPE          9\n",
       "AFFILIATION               6\n",
       "CLASSIFICATION            6\n",
       "USE_CASE                  5\n",
       "ORGANIZATION              4\n",
       "INCOME_AMT                9\n",
       "SPECIAL_CONSIDERATIONS    2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of unique values in each column\n",
    "application_df[application_cat].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APPLICATION_TYPE_Other</th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T3</th>\n",
       "      <th>APPLICATION_TYPE_T4</th>\n",
       "      <th>APPLICATION_TYPE_T5</th>\n",
       "      <th>APPLICATION_TYPE_T6</th>\n",
       "      <th>APPLICATION_TYPE_T7</th>\n",
       "      <th>APPLICATION_TYPE_T8</th>\n",
       "      <th>AFFILIATION_CompanySponsored</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    APPLICATION_TYPE_Other  APPLICATION_TYPE_T10  APPLICATION_TYPE_T19  \\\n",
       "0                      0.0                   1.0                   0.0   \n",
       "1                      0.0                   0.0                   0.0   \n",
       "2                      0.0                   0.0                   0.0   \n",
       "3                      0.0                   0.0                   0.0   \n",
       "4                      0.0                   0.0                   0.0   \n",
       "5                      0.0                   0.0                   0.0   \n",
       "6                      0.0                   0.0                   0.0   \n",
       "7                      0.0                   0.0                   0.0   \n",
       "8                      0.0                   0.0                   0.0   \n",
       "9                      0.0                   0.0                   0.0   \n",
       "10                     0.0                   0.0                   0.0   \n",
       "11                     0.0                   0.0                   0.0   \n",
       "12                     0.0                   0.0                   0.0   \n",
       "13                     0.0                   0.0                   0.0   \n",
       "14                     0.0                   0.0                   0.0   \n",
       "15                     0.0                   0.0                   0.0   \n",
       "16                     0.0                   0.0                   0.0   \n",
       "17                     0.0                   0.0                   0.0   \n",
       "18                     0.0                   0.0                   0.0   \n",
       "19                     0.0                   0.0                   0.0   \n",
       "\n",
       "    APPLICATION_TYPE_T3  APPLICATION_TYPE_T4  APPLICATION_TYPE_T5  \\\n",
       "0                   0.0                  0.0                  0.0   \n",
       "1                   1.0                  0.0                  0.0   \n",
       "2                   0.0                  0.0                  1.0   \n",
       "3                   1.0                  0.0                  0.0   \n",
       "4                   1.0                  0.0                  0.0   \n",
       "5                   1.0                  0.0                  0.0   \n",
       "6                   1.0                  0.0                  0.0   \n",
       "7                   1.0                  0.0                  0.0   \n",
       "8                   0.0                  0.0                  0.0   \n",
       "9                   0.0                  0.0                  1.0   \n",
       "10                  1.0                  0.0                  0.0   \n",
       "11                  1.0                  0.0                  0.0   \n",
       "12                  1.0                  0.0                  0.0   \n",
       "13                  1.0                  0.0                  0.0   \n",
       "14                  1.0                  0.0                  0.0   \n",
       "15                  1.0                  0.0                  0.0   \n",
       "16                  1.0                  0.0                  0.0   \n",
       "17                  1.0                  0.0                  0.0   \n",
       "18                  1.0                  0.0                  0.0   \n",
       "19                  1.0                  0.0                  0.0   \n",
       "\n",
       "    APPLICATION_TYPE_T6  APPLICATION_TYPE_T7  APPLICATION_TYPE_T8  \\\n",
       "0                   0.0                  0.0                  0.0   \n",
       "1                   0.0                  0.0                  0.0   \n",
       "2                   0.0                  0.0                  0.0   \n",
       "3                   0.0                  0.0                  0.0   \n",
       "4                   0.0                  0.0                  0.0   \n",
       "5                   0.0                  0.0                  0.0   \n",
       "6                   0.0                  0.0                  0.0   \n",
       "7                   0.0                  0.0                  0.0   \n",
       "8                   0.0                  1.0                  0.0   \n",
       "9                   0.0                  0.0                  0.0   \n",
       "10                  0.0                  0.0                  0.0   \n",
       "11                  0.0                  0.0                  0.0   \n",
       "12                  0.0                  0.0                  0.0   \n",
       "13                  0.0                  0.0                  0.0   \n",
       "14                  0.0                  0.0                  0.0   \n",
       "15                  0.0                  0.0                  0.0   \n",
       "16                  0.0                  0.0                  0.0   \n",
       "17                  0.0                  0.0                  0.0   \n",
       "18                  0.0                  0.0                  0.0   \n",
       "19                  0.0                  0.0                  0.0   \n",
       "\n",
       "    AFFILIATION_CompanySponsored  ...  INCOME_AMT_1-9999  \\\n",
       "0                            0.0  ...                0.0   \n",
       "1                            0.0  ...                1.0   \n",
       "2                            1.0  ...                0.0   \n",
       "3                            1.0  ...                0.0   \n",
       "4                            0.0  ...                0.0   \n",
       "5                            0.0  ...                0.0   \n",
       "6                            0.0  ...                0.0   \n",
       "7                            0.0  ...                0.0   \n",
       "8                            0.0  ...                1.0   \n",
       "9                            1.0  ...                0.0   \n",
       "10                           0.0  ...                0.0   \n",
       "11                           0.0  ...                0.0   \n",
       "12                           1.0  ...                0.0   \n",
       "13                           0.0  ...                0.0   \n",
       "14                           0.0  ...                0.0   \n",
       "15                           0.0  ...                0.0   \n",
       "16                           0.0  ...                0.0   \n",
       "17                           0.0  ...                0.0   \n",
       "18                           1.0  ...                0.0   \n",
       "19                           1.0  ...                0.0   \n",
       "\n",
       "    INCOME_AMT_10000-24999  INCOME_AMT_100000-499999  INCOME_AMT_10M-50M  \\\n",
       "0                      0.0                       0.0                 0.0   \n",
       "1                      0.0                       0.0                 0.0   \n",
       "2                      0.0                       0.0                 0.0   \n",
       "3                      1.0                       0.0                 0.0   \n",
       "4                      0.0                       1.0                 0.0   \n",
       "5                      0.0                       0.0                 0.0   \n",
       "6                      0.0                       1.0                 0.0   \n",
       "7                      0.0                       0.0                 1.0   \n",
       "8                      0.0                       0.0                 0.0   \n",
       "9                      0.0                       0.0                 0.0   \n",
       "10                     0.0                       0.0                 0.0   \n",
       "11                     0.0                       1.0                 0.0   \n",
       "12                     0.0                       0.0                 0.0   \n",
       "13                     0.0                       0.0                 0.0   \n",
       "14                     0.0                       0.0                 0.0   \n",
       "15                     0.0                       0.0                 0.0   \n",
       "16                     0.0                       0.0                 0.0   \n",
       "17                     0.0                       0.0                 0.0   \n",
       "18                     0.0                       0.0                 0.0   \n",
       "19                     0.0                       1.0                 0.0   \n",
       "\n",
       "    INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  INCOME_AMT_50M+  \\\n",
       "0                0.0                     0.0              0.0   \n",
       "1                0.0                     0.0              0.0   \n",
       "2                0.0                     0.0              0.0   \n",
       "3                0.0                     0.0              0.0   \n",
       "4                0.0                     0.0              0.0   \n",
       "5                0.0                     0.0              0.0   \n",
       "6                0.0                     0.0              0.0   \n",
       "7                0.0                     0.0              0.0   \n",
       "8                0.0                     0.0              0.0   \n",
       "9                0.0                     0.0              0.0   \n",
       "10               0.0                     1.0              0.0   \n",
       "11               0.0                     0.0              0.0   \n",
       "12               0.0                     0.0              0.0   \n",
       "13               0.0                     1.0              0.0   \n",
       "14               0.0                     0.0              0.0   \n",
       "15               0.0                     0.0              0.0   \n",
       "16               0.0                     0.0              0.0   \n",
       "17               0.0                     0.0              1.0   \n",
       "18               0.0                     0.0              0.0   \n",
       "19               0.0                     0.0              0.0   \n",
       "\n",
       "    INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  SPECIAL_CONSIDERATIONS_Y  \n",
       "0                 0.0                       1.0                       0.0  \n",
       "1                 0.0                       1.0                       0.0  \n",
       "2                 0.0                       1.0                       0.0  \n",
       "3                 0.0                       1.0                       0.0  \n",
       "4                 0.0                       1.0                       0.0  \n",
       "5                 0.0                       1.0                       0.0  \n",
       "6                 0.0                       1.0                       0.0  \n",
       "7                 0.0                       1.0                       0.0  \n",
       "8                 0.0                       1.0                       0.0  \n",
       "9                 0.0                       1.0                       0.0  \n",
       "10                0.0                       1.0                       0.0  \n",
       "11                0.0                       1.0                       0.0  \n",
       "12                0.0                       1.0                       0.0  \n",
       "13                0.0                       1.0                       0.0  \n",
       "14                0.0                       1.0                       0.0  \n",
       "15                0.0                       1.0                       0.0  \n",
       "16                0.0                       1.0                       0.0  \n",
       "17                0.0                       1.0                       0.0  \n",
       "18                0.0                       1.0                       0.0  \n",
       "19                0.0                       1.0                       0.0  \n",
       "\n",
       "[20 rows x 41 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a OneHotEncoder instance\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(application_df[application_cat]))\n",
    "\n",
    "# Add the encoded variable names to the dataframe\n",
    "encode_df.columns = enc.get_feature_names_out(application_cat)\n",
    "encode_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WangTech\\AppData\\Local\\Temp\\ipykernel_47784\\4109288422.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  application_df = application_df.drop(application_cat,1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATUS</th>\n",
       "      <th>ASK_AMT</th>\n",
       "      <th>IS_SUCCESSFUL</th>\n",
       "      <th>APPLICATION_TYPE_Other</th>\n",
       "      <th>APPLICATION_TYPE_T10</th>\n",
       "      <th>APPLICATION_TYPE_T19</th>\n",
       "      <th>APPLICATION_TYPE_T3</th>\n",
       "      <th>APPLICATION_TYPE_T4</th>\n",
       "      <th>APPLICATION_TYPE_T5</th>\n",
       "      <th>APPLICATION_TYPE_T6</th>\n",
       "      <th>...</th>\n",
       "      <th>INCOME_AMT_1-9999</th>\n",
       "      <th>INCOME_AMT_10000-24999</th>\n",
       "      <th>INCOME_AMT_100000-499999</th>\n",
       "      <th>INCOME_AMT_10M-50M</th>\n",
       "      <th>INCOME_AMT_1M-5M</th>\n",
       "      <th>INCOME_AMT_25000-99999</th>\n",
       "      <th>INCOME_AMT_50M+</th>\n",
       "      <th>INCOME_AMT_5M-10M</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_N</th>\n",
       "      <th>SPECIAL_CONSIDERATIONS_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34279</th>\n",
       "      <td>1</td>\n",
       "      <td>11442</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34280</th>\n",
       "      <td>1</td>\n",
       "      <td>401661</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34281</th>\n",
       "      <td>1</td>\n",
       "      <td>56261</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34282</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34283</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34284</th>\n",
       "      <td>1</td>\n",
       "      <td>3384856</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34285</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34286</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34287</th>\n",
       "      <td>1</td>\n",
       "      <td>6713</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34288</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34289</th>\n",
       "      <td>1</td>\n",
       "      <td>20197</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34290</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34291</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34292</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34293</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34294</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34295</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34296</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34297</th>\n",
       "      <td>1</td>\n",
       "      <td>5000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34298</th>\n",
       "      <td>1</td>\n",
       "      <td>36500179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows  44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       STATUS   ASK_AMT  IS_SUCCESSFUL  APPLICATION_TYPE_Other  \\\n",
       "34279       1     11442              1                     0.0   \n",
       "34280       1    401661              0                     0.0   \n",
       "34281       1     56261              1                     0.0   \n",
       "34282       1      5000              0                     0.0   \n",
       "34283       1      5000              0                     0.0   \n",
       "34284       1   3384856              0                     0.0   \n",
       "34285       1      5000              1                     0.0   \n",
       "34286       1      5000              0                     0.0   \n",
       "34287       1      6713              1                     0.0   \n",
       "34288       1      5000              0                     0.0   \n",
       "34289       1     20197              1                     0.0   \n",
       "34290       1      5000              0                     0.0   \n",
       "34291       1      5000              0                     0.0   \n",
       "34292       1      5000              0                     0.0   \n",
       "34293       1      5000              1                     0.0   \n",
       "34294       1      5000              0                     0.0   \n",
       "34295       1      5000              0                     0.0   \n",
       "34296       1      5000              0                     0.0   \n",
       "34297       1      5000              1                     0.0   \n",
       "34298       1  36500179              0                     0.0   \n",
       "\n",
       "       APPLICATION_TYPE_T10  APPLICATION_TYPE_T19  APPLICATION_TYPE_T3  \\\n",
       "34279                   0.0                   0.0                  1.0   \n",
       "34280                   0.0                   0.0                  1.0   \n",
       "34281                   0.0                   0.0                  1.0   \n",
       "34282                   0.0                   0.0                  0.0   \n",
       "34283                   0.0                   0.0                  0.0   \n",
       "34284                   0.0                   0.0                  1.0   \n",
       "34285                   0.0                   0.0                  1.0   \n",
       "34286                   0.0                   0.0                  1.0   \n",
       "34287                   0.0                   0.0                  1.0   \n",
       "34288                   0.0                   0.0                  1.0   \n",
       "34289                   0.0                   0.0                  1.0   \n",
       "34290                   0.0                   0.0                  0.0   \n",
       "34291                   0.0                   0.0                  0.0   \n",
       "34292                   0.0                   0.0                  0.0   \n",
       "34293                   0.0                   0.0                  1.0   \n",
       "34294                   0.0                   0.0                  0.0   \n",
       "34295                   0.0                   0.0                  0.0   \n",
       "34296                   0.0                   0.0                  1.0   \n",
       "34297                   0.0                   0.0                  0.0   \n",
       "34298                   0.0                   0.0                  1.0   \n",
       "\n",
       "       APPLICATION_TYPE_T4  APPLICATION_TYPE_T5  APPLICATION_TYPE_T6  ...  \\\n",
       "34279                  0.0                  0.0                  0.0  ...   \n",
       "34280                  0.0                  0.0                  0.0  ...   \n",
       "34281                  0.0                  0.0                  0.0  ...   \n",
       "34282                  0.0                  0.0                  1.0  ...   \n",
       "34283                  0.0                  0.0                  1.0  ...   \n",
       "34284                  0.0                  0.0                  0.0  ...   \n",
       "34285                  0.0                  0.0                  0.0  ...   \n",
       "34286                  0.0                  0.0                  0.0  ...   \n",
       "34287                  0.0                  0.0                  0.0  ...   \n",
       "34288                  0.0                  0.0                  0.0  ...   \n",
       "34289                  0.0                  0.0                  0.0  ...   \n",
       "34290                  1.0                  0.0                  0.0  ...   \n",
       "34291                  1.0                  0.0                  0.0  ...   \n",
       "34292                  1.0                  0.0                  0.0  ...   \n",
       "34293                  0.0                  0.0                  0.0  ...   \n",
       "34294                  1.0                  0.0                  0.0  ...   \n",
       "34295                  1.0                  0.0                  0.0  ...   \n",
       "34296                  0.0                  0.0                  0.0  ...   \n",
       "34297                  0.0                  1.0                  0.0  ...   \n",
       "34298                  0.0                  0.0                  0.0  ...   \n",
       "\n",
       "       INCOME_AMT_1-9999  INCOME_AMT_10000-24999  INCOME_AMT_100000-499999  \\\n",
       "34279                0.0                     0.0                       0.0   \n",
       "34280                0.0                     0.0                       1.0   \n",
       "34281                0.0                     0.0                       0.0   \n",
       "34282                0.0                     0.0                       0.0   \n",
       "34283                0.0                     0.0                       0.0   \n",
       "34284                0.0                     0.0                       0.0   \n",
       "34285                0.0                     0.0                       0.0   \n",
       "34286                0.0                     0.0                       0.0   \n",
       "34287                0.0                     0.0                       0.0   \n",
       "34288                0.0                     0.0                       0.0   \n",
       "34289                0.0                     0.0                       0.0   \n",
       "34290                1.0                     0.0                       0.0   \n",
       "34291                0.0                     0.0                       0.0   \n",
       "34292                0.0                     0.0                       0.0   \n",
       "34293                0.0                     0.0                       0.0   \n",
       "34294                0.0                     0.0                       0.0   \n",
       "34295                0.0                     0.0                       0.0   \n",
       "34296                0.0                     0.0                       0.0   \n",
       "34297                0.0                     0.0                       0.0   \n",
       "34298                0.0                     0.0                       0.0   \n",
       "\n",
       "       INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  \\\n",
       "34279                 0.0               0.0                     1.0   \n",
       "34280                 0.0               0.0                     0.0   \n",
       "34281                 0.0               1.0                     0.0   \n",
       "34282                 0.0               0.0                     0.0   \n",
       "34283                 0.0               0.0                     0.0   \n",
       "34284                 0.0               1.0                     0.0   \n",
       "34285                 0.0               0.0                     0.0   \n",
       "34286                 0.0               0.0                     0.0   \n",
       "34287                 0.0               0.0                     1.0   \n",
       "34288                 0.0               0.0                     0.0   \n",
       "34289                 0.0               0.0                     1.0   \n",
       "34290                 0.0               0.0                     0.0   \n",
       "34291                 0.0               0.0                     0.0   \n",
       "34292                 0.0               0.0                     0.0   \n",
       "34293                 0.0               0.0                     0.0   \n",
       "34294                 0.0               0.0                     0.0   \n",
       "34295                 0.0               0.0                     0.0   \n",
       "34296                 0.0               0.0                     0.0   \n",
       "34297                 0.0               0.0                     0.0   \n",
       "34298                 0.0               1.0                     0.0   \n",
       "\n",
       "       INCOME_AMT_50M+  INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  \\\n",
       "34279              0.0                0.0                       1.0   \n",
       "34280              0.0                0.0                       1.0   \n",
       "34281              0.0                0.0                       1.0   \n",
       "34282              0.0                0.0                       1.0   \n",
       "34283              0.0                0.0                       1.0   \n",
       "34284              0.0                0.0                       1.0   \n",
       "34285              0.0                0.0                       1.0   \n",
       "34286              0.0                0.0                       1.0   \n",
       "34287              0.0                0.0                       1.0   \n",
       "34288              0.0                0.0                       1.0   \n",
       "34289              0.0                0.0                       1.0   \n",
       "34290              0.0                0.0                       1.0   \n",
       "34291              0.0                0.0                       1.0   \n",
       "34292              0.0                0.0                       1.0   \n",
       "34293              0.0                0.0                       1.0   \n",
       "34294              0.0                0.0                       1.0   \n",
       "34295              0.0                0.0                       1.0   \n",
       "34296              0.0                0.0                       1.0   \n",
       "34297              0.0                0.0                       1.0   \n",
       "34298              0.0                0.0                       1.0   \n",
       "\n",
       "       SPECIAL_CONSIDERATIONS_Y  \n",
       "34279                       0.0  \n",
       "34280                       0.0  \n",
       "34281                       0.0  \n",
       "34282                       0.0  \n",
       "34283                       0.0  \n",
       "34284                       0.0  \n",
       "34285                       0.0  \n",
       "34286                       0.0  \n",
       "34287                       0.0  \n",
       "34288                       0.0  \n",
       "34289                       0.0  \n",
       "34290                       0.0  \n",
       "34291                       0.0  \n",
       "34292                       0.0  \n",
       "34293                       0.0  \n",
       "34294                       0.0  \n",
       "34295                       0.0  \n",
       "34296                       0.0  \n",
       "34297                       0.0  \n",
       "34298                       0.0  \n",
       "\n",
       "[20 rows x 44 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge one-hot encoded features and drop the originals\n",
    "application_df = application_df.merge(encode_df,left_index=True, right_index=True)\n",
    "application_df = application_df.drop(application_cat,1)\n",
    "application_df.tail(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1 - Drop ASK_AMT and STATUS columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example outlier plot of ASK_AMT\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.set_title('Ask Amounts at Loan Application')\n",
    "ax1.set_ylabel('Ask Amount')\n",
    "ax1.boxplot(application_df[['ASK_AMT']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ASK_AMT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.429900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.769199e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.713045e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.000000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.742000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8.597806e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ASK_AMT\n",
       "count  3.429900e+04\n",
       "mean   2.769199e+06\n",
       "std    8.713045e+07\n",
       "min    5.000000e+03\n",
       "25%    5.000000e+03\n",
       "50%    5.000000e+03\n",
       "75%    7.742000e+03\n",
       "max    8.597806e+09"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine which data points are outside of the 1.5*IQR range\n",
    "quartiles = np.quantile(application_df[['ASK_AMT']],[.25,.75])\n",
    "iqr = quartiles[1]-quartiles[0]\n",
    "lower_bound = quartiles[0]-(1.5*iqr)\n",
    "upper_bound = quartiles[1]+(1.5*iqr)\n",
    "mode_amt = application_df[['ASK_AMT']].mode()\n",
    "mean_amt = application_df[['ASK_AMT']].mean()\n",
    "\n",
    "application_df[['ASK_AMT']].describe()\n",
    "\n",
    "# Print the the potential outliers\n",
    "# potential_outliers = [print(f'{amt}') if amt <= lower_bound or amt >= upper_bound else next for amt in application_df[['ASK_AMT']].values]\n",
    "# print(f'The Mode is: {mode_amt}')\n",
    "# print(f'The Mean is: {mean_amt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt_1_df = application_df.drop(columns=['ASK_AMT','STATUS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WangTech\\AppData\\Local\\Temp\\ipykernel_47784\\1187756357.py:3: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X = attempt_1_df.drop([\"IS_SUCCESSFUL\"],1).values\n"
     ]
    }
   ],
   "source": [
    "# Split our preprocessed data into our features and target arrays\n",
    "y = attempt_1_df[\"IS_SUCCESSFUL\"].values.reshape(-1,1)\n",
    "X = attempt_1_df.drop([\"IS_SUCCESSFUL\"],1).values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile, Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 80)                3360      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                2430      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,821\n",
      "Trainable params: 5,821\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 80\n",
    "hidden_nodes_layer2 = 30\n",
    "hidden_nodes_layer3 = 30\n",
    "hidden_nodes_layer4 = 15\n",
    "hidden_nodes_layer5 = 30\n",
    "hidden_nodes_layer6 = 10\n",
    "hidden_nodes_layer7 = 5\n",
    "output_features = len(application_df[[\"IS_SUCCESSFUL\"]].columns)\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# # Third hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "# # Fourth hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"relu\"))\n",
    "\n",
    "# # Fifth hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer5, activation=\"tanh\"))\n",
    "\n",
    "# # Sixth hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer6, activation=\"tanh\"))\n",
    "\n",
    "# # Seventh hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer7, activation=\"tanh\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=output_features, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import checkpoint dependencies\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"optimization_checkpoints/\",exist_ok=True)\n",
    "checkpoint_path = \"optimization_checkpoints/attempt_1_weights.{epoch:03d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq = 'epoch',\n",
    "    period=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "804/804 [==============================] - 2s 1ms/step - loss: 0.5713 - accuracy: 0.7219\n",
      "Epoch 2/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5556 - accuracy: 0.7296\n",
      "Epoch 3/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5523 - accuracy: 0.7323\n",
      "Epoch 4/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5503 - accuracy: 0.7327\n",
      "Epoch 5/100\n",
      "798/804 [============================>.] - ETA: 0s - loss: 0.5491 - accuracy: 0.7332\n",
      "Epoch 5: saving model to optimization_checkpoints\\attempt_1_weights.005.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5493 - accuracy: 0.7331\n",
      "Epoch 6/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5488 - accuracy: 0.7348\n",
      "Epoch 7/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5484 - accuracy: 0.7332\n",
      "Epoch 8/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5477 - accuracy: 0.7355\n",
      "Epoch 9/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5469 - accuracy: 0.7341\n",
      "Epoch 10/100\n",
      "786/804 [============================>.] - ETA: 0s - loss: 0.5463 - accuracy: 0.7346\n",
      "Epoch 10: saving model to optimization_checkpoints\\attempt_1_weights.010.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5463 - accuracy: 0.7348\n",
      "Epoch 11/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5460 - accuracy: 0.7352\n",
      "Epoch 12/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5456 - accuracy: 0.7356\n",
      "Epoch 13/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5457 - accuracy: 0.7364\n",
      "Epoch 14/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5450 - accuracy: 0.7363\n",
      "Epoch 15/100\n",
      "782/804 [============================>.] - ETA: 0s - loss: 0.5448 - accuracy: 0.7352\n",
      "Epoch 15: saving model to optimization_checkpoints\\attempt_1_weights.015.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5449 - accuracy: 0.7353\n",
      "Epoch 16/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5443 - accuracy: 0.7358\n",
      "Epoch 17/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5438 - accuracy: 0.7362\n",
      "Epoch 18/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5435 - accuracy: 0.7367\n",
      "Epoch 19/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5432 - accuracy: 0.7364\n",
      "Epoch 20/100\n",
      "803/804 [============================>.] - ETA: 0s - loss: 0.5433 - accuracy: 0.7372\n",
      "Epoch 20: saving model to optimization_checkpoints\\attempt_1_weights.020.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5434 - accuracy: 0.7371\n",
      "Epoch 21/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5428 - accuracy: 0.7367\n",
      "Epoch 22/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5423 - accuracy: 0.7369\n",
      "Epoch 23/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5427 - accuracy: 0.7364\n",
      "Epoch 24/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5422 - accuracy: 0.7374\n",
      "Epoch 25/100\n",
      "785/804 [============================>.] - ETA: 0s - loss: 0.5418 - accuracy: 0.7378\n",
      "Epoch 25: saving model to optimization_checkpoints\\attempt_1_weights.025.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5419 - accuracy: 0.7375\n",
      "Epoch 26/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5416 - accuracy: 0.7378\n",
      "Epoch 27/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5415 - accuracy: 0.7374\n",
      "Epoch 28/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5420 - accuracy: 0.7385\n",
      "Epoch 29/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5418 - accuracy: 0.7371\n",
      "Epoch 30/100\n",
      "775/804 [===========================>..] - ETA: 0s - loss: 0.5418 - accuracy: 0.7382\n",
      "Epoch 30: saving model to optimization_checkpoints\\attempt_1_weights.030.hdf5\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7381\n",
      "Epoch 31/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5410 - accuracy: 0.7378\n",
      "Epoch 32/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5407 - accuracy: 0.7387\n",
      "Epoch 33/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5405 - accuracy: 0.7383\n",
      "Epoch 34/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5409 - accuracy: 0.7380\n",
      "Epoch 35/100\n",
      "804/804 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.7386\n",
      "Epoch 35: saving model to optimization_checkpoints\\attempt_1_weights.035.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5408 - accuracy: 0.7386\n",
      "Epoch 36/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5406 - accuracy: 0.7395\n",
      "Epoch 37/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7384\n",
      "Epoch 38/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5399 - accuracy: 0.7386\n",
      "Epoch 39/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7388\n",
      "Epoch 40/100\n",
      "776/804 [===========================>..] - ETA: 0s - loss: 0.5395 - accuracy: 0.7392\n",
      "Epoch 40: saving model to optimization_checkpoints\\attempt_1_weights.040.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5396 - accuracy: 0.7393\n",
      "Epoch 41/100\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5397 - accuracy: 0.7399\n",
      "Epoch 42/100\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5401 - accuracy: 0.7386\n",
      "Epoch 43/100\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7390\n",
      "Epoch 44/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5396 - accuracy: 0.7397\n",
      "Epoch 45/100\n",
      "783/804 [============================>.] - ETA: 0s - loss: 0.5392 - accuracy: 0.7399\n",
      "Epoch 45: saving model to optimization_checkpoints\\attempt_1_weights.045.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5395 - accuracy: 0.7395\n",
      "Epoch 46/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5392 - accuracy: 0.7395\n",
      "Epoch 47/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5389 - accuracy: 0.7400\n",
      "Epoch 48/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5389 - accuracy: 0.7399\n",
      "Epoch 49/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5387 - accuracy: 0.7395\n",
      "Epoch 50/100\n",
      "787/804 [============================>.] - ETA: 0s - loss: 0.5399 - accuracy: 0.7393\n",
      "Epoch 50: saving model to optimization_checkpoints\\attempt_1_weights.050.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5390 - accuracy: 0.7400\n",
      "Epoch 51/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5392 - accuracy: 0.7399\n",
      "Epoch 52/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5389 - accuracy: 0.7395\n",
      "Epoch 53/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5388 - accuracy: 0.7408\n",
      "Epoch 54/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5385 - accuracy: 0.7390\n",
      "Epoch 55/100\n",
      "774/804 [===========================>..] - ETA: 0s - loss: 0.5385 - accuracy: 0.7403\n",
      "Epoch 55: saving model to optimization_checkpoints\\attempt_1_weights.055.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5384 - accuracy: 0.7403\n",
      "Epoch 56/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5385 - accuracy: 0.7398\n",
      "Epoch 57/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5383 - accuracy: 0.7402\n",
      "Epoch 58/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5378 - accuracy: 0.7407\n",
      "Epoch 59/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5380 - accuracy: 0.7397\n",
      "Epoch 60/100\n",
      "773/804 [===========================>..] - ETA: 0s - loss: 0.5382 - accuracy: 0.7403\n",
      "Epoch 60: saving model to optimization_checkpoints\\attempt_1_weights.060.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5382 - accuracy: 0.7399\n",
      "Epoch 61/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5378 - accuracy: 0.7401\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5379 - accuracy: 0.7402\n",
      "Epoch 63/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5381 - accuracy: 0.7405\n",
      "Epoch 64/100\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5378 - accuracy: 0.7404\n",
      "Epoch 65/100\n",
      "793/804 [============================>.] - ETA: 0s - loss: 0.5381 - accuracy: 0.7392\n",
      "Epoch 65: saving model to optimization_checkpoints\\attempt_1_weights.065.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5379 - accuracy: 0.7395\n",
      "Epoch 66/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5376 - accuracy: 0.7404\n",
      "Epoch 67/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5376 - accuracy: 0.7414\n",
      "Epoch 68/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5377 - accuracy: 0.7402\n",
      "Epoch 69/100\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5378 - accuracy: 0.7398\n",
      "Epoch 70/100\n",
      "771/804 [===========================>..] - ETA: 0s - loss: 0.5388 - accuracy: 0.7393\n",
      "Epoch 70: saving model to optimization_checkpoints\\attempt_1_weights.070.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5373 - accuracy: 0.7406\n",
      "Epoch 71/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5375 - accuracy: 0.7405\n",
      "Epoch 72/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5378 - accuracy: 0.7400\n",
      "Epoch 73/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5374 - accuracy: 0.7402\n",
      "Epoch 74/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5376 - accuracy: 0.7404\n",
      "Epoch 75/100\n",
      "781/804 [============================>.] - ETA: 0s - loss: 0.5379 - accuracy: 0.7401\n",
      "Epoch 75: saving model to optimization_checkpoints\\attempt_1_weights.075.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5380 - accuracy: 0.7399\n",
      "Epoch 76/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5372 - accuracy: 0.7406\n",
      "Epoch 77/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5371 - accuracy: 0.7398\n",
      "Epoch 78/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5373 - accuracy: 0.7410\n",
      "Epoch 79/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5372 - accuracy: 0.7406\n",
      "Epoch 80/100\n",
      "804/804 [==============================] - ETA: 0s - loss: 0.5375 - accuracy: 0.7404\n",
      "Epoch 80: saving model to optimization_checkpoints\\attempt_1_weights.080.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5375 - accuracy: 0.7404\n",
      "Epoch 81/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5370 - accuracy: 0.7393\n",
      "Epoch 82/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5372 - accuracy: 0.7409\n",
      "Epoch 83/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5368 - accuracy: 0.7401\n",
      "Epoch 84/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5367 - accuracy: 0.7409\n",
      "Epoch 85/100\n",
      "791/804 [============================>.] - ETA: 0s - loss: 0.5373 - accuracy: 0.7401\n",
      "Epoch 85: saving model to optimization_checkpoints\\attempt_1_weights.085.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5369 - accuracy: 0.7405\n",
      "Epoch 86/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5367 - accuracy: 0.7402\n",
      "Epoch 87/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5365 - accuracy: 0.7409\n",
      "Epoch 88/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5369 - accuracy: 0.7400\n",
      "Epoch 89/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5365 - accuracy: 0.7404\n",
      "Epoch 90/100\n",
      "796/804 [============================>.] - ETA: 0s - loss: 0.5364 - accuracy: 0.7403\n",
      "Epoch 90: saving model to optimization_checkpoints\\attempt_1_weights.090.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5365 - accuracy: 0.7401\n",
      "Epoch 91/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5366 - accuracy: 0.7403\n",
      "Epoch 92/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5365 - accuracy: 0.7401\n",
      "Epoch 93/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5366 - accuracy: 0.7406\n",
      "Epoch 94/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5362 - accuracy: 0.7402\n",
      "Epoch 95/100\n",
      "779/804 [============================>.] - ETA: 0s - loss: 0.5372 - accuracy: 0.7397\n",
      "Epoch 95: saving model to optimization_checkpoints\\attempt_1_weights.095.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5365 - accuracy: 0.7401\n",
      "Epoch 96/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5359 - accuracy: 0.7405\n",
      "Epoch 97/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5367 - accuracy: 0.7403\n",
      "Epoch 98/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5359 - accuracy: 0.7409\n",
      "Epoch 99/100\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5363 - accuracy: 0.7405\n",
      "Epoch 100/100\n",
      "779/804 [============================>.] - ETA: 0s - loss: 0.5366 - accuracy: 0.7400\n",
      "Epoch 100: saving model to optimization_checkpoints\\attempt_1_weights.100.hdf5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5363 - accuracy: 0.7404\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=100,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 - 0s - loss: 0.5566 - accuracy: 0.7248 - 435ms/epoch - 2ms/step\n",
      "Loss: 0.5566121339797974, Accuracy: 0.724781334400177\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "nn.save(\"AlphabetSoupCharity_Optimization_attempt_1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2: Increasing the hidden layers and the number of neurons, also increasing the epoch to 200, using the same preprocessed data from Attempt 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 160)               6880      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 120)               19320     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 80)                9680      \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 40)                3240      \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 39,161\n",
      "Trainable params: 39,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 160\n",
    "hidden_nodes_layer2 = 120\n",
    "hidden_nodes_layer3 = 80\n",
    "hidden_nodes_layer4 = 40\n",
    "hidden_nodes_layer5 = 30\n",
    "hidden_nodes_layer6 = 10\n",
    "output_features = len(application_df[[\"IS_SUCCESSFUL\"]].columns)\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n",
    "\n",
    "# Fourth hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"relu\"))\n",
    "\n",
    "# # Fifth hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer5, activation=\"tanh\"))\n",
    "\n",
    "# # Sixth hidden layer\n",
    "# nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer6, activation=\"tanh\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=output_features, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import checkpoint dependencies\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"optimization_checkpoints/\",exist_ok=True)\n",
    "checkpoint_path = \"optimization_checkpoints/attempt_2_weights.{epoch:03d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq = 'epoch',\n",
    "    period=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "804/804 [==============================] - 3s 2ms/step - loss: 0.5686 - accuracy: 0.7227\n",
      "Epoch 2/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5554 - accuracy: 0.7297\n",
      "Epoch 3/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5527 - accuracy: 0.7321\n",
      "Epoch 4/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5509 - accuracy: 0.7327\n",
      "Epoch 5/200\n",
      "791/804 [============================>.] - ETA: 0s - loss: 0.5505 - accuracy: 0.7324\n",
      "Epoch 5: saving model to optimization_checkpoints\\attempt_2_weights.005.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5504 - accuracy: 0.7324\n",
      "Epoch 6/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5489 - accuracy: 0.7345\n",
      "Epoch 7/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5484 - accuracy: 0.7355\n",
      "Epoch 8/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5472 - accuracy: 0.7353\n",
      "Epoch 9/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5468 - accuracy: 0.7365\n",
      "Epoch 10/200\n",
      "782/804 [============================>.] - ETA: 0s - loss: 0.5454 - accuracy: 0.7357\n",
      "Epoch 10: saving model to optimization_checkpoints\\attempt_2_weights.010.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5455 - accuracy: 0.7354\n",
      "Epoch 11/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5456 - accuracy: 0.7362\n",
      "Epoch 12/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5446 - accuracy: 0.7364\n",
      "Epoch 13/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5446 - accuracy: 0.7371\n",
      "Epoch 14/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5441 - accuracy: 0.7367\n",
      "Epoch 15/200\n",
      "790/804 [============================>.] - ETA: 0s - loss: 0.5438 - accuracy: 0.7381\n",
      "Epoch 15: saving model to optimization_checkpoints\\attempt_2_weights.015.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5435 - accuracy: 0.7381\n",
      "Epoch 16/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5435 - accuracy: 0.7370\n",
      "Epoch 17/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5432 - accuracy: 0.7380\n",
      "Epoch 18/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5426 - accuracy: 0.7366\n",
      "Epoch 19/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5421 - accuracy: 0.7380\n",
      "Epoch 20/200\n",
      "792/804 [============================>.] - ETA: 0s - loss: 0.5422 - accuracy: 0.7388\n",
      "Epoch 20: saving model to optimization_checkpoints\\attempt_2_weights.020.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5418 - accuracy: 0.7388\n",
      "Epoch 21/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5416 - accuracy: 0.7390\n",
      "Epoch 22/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5411 - accuracy: 0.7382\n",
      "Epoch 23/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5411 - accuracy: 0.7386\n",
      "Epoch 24/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5403 - accuracy: 0.7390\n",
      "Epoch 25/200\n",
      "793/804 [============================>.] - ETA: 0s - loss: 0.5405 - accuracy: 0.7388\n",
      "Epoch 25: saving model to optimization_checkpoints\\attempt_2_weights.025.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5403 - accuracy: 0.7389\n",
      "Epoch 26/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5402 - accuracy: 0.7395\n",
      "Epoch 27/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5404 - accuracy: 0.7391\n",
      "Epoch 28/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.7392\n",
      "Epoch 29/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5392 - accuracy: 0.7397\n",
      "Epoch 30/200\n",
      "794/804 [============================>.] - ETA: 0s - loss: 0.5393 - accuracy: 0.7400\n",
      "Epoch 30: saving model to optimization_checkpoints\\attempt_2_weights.030.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7400\n",
      "Epoch 31/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5396 - accuracy: 0.7397\n",
      "Epoch 32/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5393 - accuracy: 0.7389\n",
      "Epoch 33/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7400\n",
      "Epoch 34/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5388 - accuracy: 0.7397\n",
      "Epoch 35/200\n",
      "781/804 [============================>.] - ETA: 0s - loss: 0.5391 - accuracy: 0.7399\n",
      "Epoch 35: saving model to optimization_checkpoints\\attempt_2_weights.035.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5385 - accuracy: 0.7404\n",
      "Epoch 36/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5382 - accuracy: 0.7400\n",
      "Epoch 37/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5386 - accuracy: 0.7400\n",
      "Epoch 38/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5378 - accuracy: 0.7402\n",
      "Epoch 39/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5385 - accuracy: 0.7392\n",
      "Epoch 40/200\n",
      "781/804 [============================>.] - ETA: 0s - loss: 0.5368 - accuracy: 0.7403\n",
      "Epoch 40: saving model to optimization_checkpoints\\attempt_2_weights.040.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5372 - accuracy: 0.7401\n",
      "Epoch 41/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5380 - accuracy: 0.7395\n",
      "Epoch 42/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5378 - accuracy: 0.7401\n",
      "Epoch 43/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5372 - accuracy: 0.7397\n",
      "Epoch 44/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5366 - accuracy: 0.7405\n",
      "Epoch 45/200\n",
      "791/804 [============================>.] - ETA: 0s - loss: 0.5376 - accuracy: 0.7406\n",
      "Epoch 45: saving model to optimization_checkpoints\\attempt_2_weights.045.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5377 - accuracy: 0.7406\n",
      "Epoch 46/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5370 - accuracy: 0.7410\n",
      "Epoch 47/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5368 - accuracy: 0.7404\n",
      "Epoch 48/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5366 - accuracy: 0.7401\n",
      "Epoch 49/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5365 - accuracy: 0.7402\n",
      "Epoch 50/200\n",
      "797/804 [============================>.] - ETA: 0s - loss: 0.5362 - accuracy: 0.7406\n",
      "Epoch 50: saving model to optimization_checkpoints\\attempt_2_weights.050.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5364 - accuracy: 0.7407\n",
      "Epoch 51/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5382 - accuracy: 0.7397\n",
      "Epoch 52/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5361 - accuracy: 0.7399\n",
      "Epoch 53/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5360 - accuracy: 0.7409\n",
      "Epoch 54/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7411\n",
      "Epoch 55/200\n",
      "779/804 [============================>.] - ETA: 0s - loss: 0.5361 - accuracy: 0.7397\n",
      "Epoch 55: saving model to optimization_checkpoints\\attempt_2_weights.055.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7400\n",
      "Epoch 56/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7400\n",
      "Epoch 57/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7404\n",
      "Epoch 58/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5362 - accuracy: 0.7407\n",
      "Epoch 59/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5357 - accuracy: 0.7407\n",
      "Epoch 60/200\n",
      "798/804 [============================>.] - ETA: 0s - loss: 0.5362 - accuracy: 0.7412\n",
      "Epoch 60: saving model to optimization_checkpoints\\attempt_2_weights.060.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5363 - accuracy: 0.7410\n",
      "Epoch 61/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5356 - accuracy: 0.7407\n",
      "Epoch 62/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7410\n",
      "Epoch 63/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5358 - accuracy: 0.7414\n",
      "Epoch 64/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5357 - accuracy: 0.7407\n",
      "Epoch 65/200\n",
      "802/804 [============================>.] - ETA: 0s - loss: 0.5360 - accuracy: 0.7412\n",
      "Epoch 65: saving model to optimization_checkpoints\\attempt_2_weights.065.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5359 - accuracy: 0.7412\n",
      "Epoch 66/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5355 - accuracy: 0.7410\n",
      "Epoch 67/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5357 - accuracy: 0.7410\n",
      "Epoch 68/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5348 - accuracy: 0.7412\n",
      "Epoch 69/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7408\n",
      "Epoch 70/200\n",
      "778/804 [============================>.] - ETA: 0s - loss: 0.5335 - accuracy: 0.7418\n",
      "Epoch 70: saving model to optimization_checkpoints\\attempt_2_weights.070.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7413\n",
      "Epoch 71/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5354 - accuracy: 0.7407\n",
      "Epoch 72/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5347 - accuracy: 0.7407\n",
      "Epoch 73/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5356 - accuracy: 0.7412\n",
      "Epoch 74/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5433 - accuracy: 0.7413\n",
      "Epoch 75/200\n",
      "792/804 [============================>.] - ETA: 0s - loss: 0.5353 - accuracy: 0.7414\n",
      "Epoch 75: saving model to optimization_checkpoints\\attempt_2_weights.075.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5356 - accuracy: 0.7411\n",
      "Epoch 76/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7409\n",
      "Epoch 77/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7408\n",
      "Epoch 78/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5343 - accuracy: 0.7413\n",
      "Epoch 79/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5340 - accuracy: 0.7415\n",
      "Epoch 80/200\n",
      "789/804 [============================>.] - ETA: 0s - loss: 0.5344 - accuracy: 0.7406\n",
      "Epoch 80: saving model to optimization_checkpoints\\attempt_2_weights.080.hdf5\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5343 - accuracy: 0.7405\n",
      "Epoch 81/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7411\n",
      "Epoch 82/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5348 - accuracy: 0.7409\n",
      "Epoch 83/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5369 - accuracy: 0.7409\n",
      "Epoch 84/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7409\n",
      "Epoch 85/200\n",
      "789/804 [============================>.] - ETA: 0s - loss: 0.5340 - accuracy: 0.7413\n",
      "Epoch 85: saving model to optimization_checkpoints\\attempt_2_weights.085.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7411\n",
      "Epoch 86/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5353 - accuracy: 0.7409\n",
      "Epoch 87/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7405\n",
      "Epoch 88/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7412\n",
      "Epoch 89/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5341 - accuracy: 0.7410\n",
      "Epoch 90/200\n",
      "778/804 [============================>.] - ETA: 0s - loss: 0.5346 - accuracy: 0.7420\n",
      "Epoch 90: saving model to optimization_checkpoints\\attempt_2_weights.090.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5353 - accuracy: 0.7410\n",
      "Epoch 91/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7411\n",
      "Epoch 92/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5339 - accuracy: 0.7414\n",
      "Epoch 93/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5337 - accuracy: 0.7413\n",
      "Epoch 94/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5341 - accuracy: 0.7413\n",
      "Epoch 95/200\n",
      "784/804 [============================>.] - ETA: 0s - loss: 0.5336 - accuracy: 0.7412\n",
      "Epoch 95: saving model to optimization_checkpoints\\attempt_2_weights.095.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7411\n",
      "Epoch 96/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5359 - accuracy: 0.7414\n",
      "Epoch 97/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7406\n",
      "Epoch 98/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5334 - accuracy: 0.7412\n",
      "Epoch 99/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5341 - accuracy: 0.7404\n",
      "Epoch 100/200\n",
      "782/804 [============================>.] - ETA: 0s - loss: 0.5333 - accuracy: 0.7414\n",
      "Epoch 100: saving model to optimization_checkpoints\\attempt_2_weights.100.hdf5\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5341 - accuracy: 0.7409\n",
      "Epoch 101/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7409\n",
      "Epoch 102/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5337 - accuracy: 0.7417\n",
      "Epoch 103/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5339 - accuracy: 0.7409\n",
      "Epoch 104/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5340 - accuracy: 0.7406\n",
      "Epoch 105/200\n",
      "799/804 [============================>.] - ETA: 0s - loss: 0.5329 - accuracy: 0.7410\n",
      "Epoch 105: saving model to optimization_checkpoints\\attempt_2_weights.105.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5332 - accuracy: 0.7409\n",
      "Epoch 106/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7414\n",
      "Epoch 107/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7413\n",
      "Epoch 108/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5335 - accuracy: 0.7411\n",
      "Epoch 109/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5330 - accuracy: 0.7413\n",
      "Epoch 110/200\n",
      "803/804 [============================>.] - ETA: 0s - loss: 0.5336 - accuracy: 0.7417\n",
      "Epoch 110: saving model to optimization_checkpoints\\attempt_2_weights.110.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5337 - accuracy: 0.7417\n",
      "Epoch 111/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5350 - accuracy: 0.7414\n",
      "Epoch 112/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7411\n",
      "Epoch 113/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5335 - accuracy: 0.7416\n",
      "Epoch 114/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7407\n",
      "Epoch 115/200\n",
      "801/804 [============================>.] - ETA: 0s - loss: 0.5330 - accuracy: 0.7411\n",
      "Epoch 115: saving model to optimization_checkpoints\\attempt_2_weights.115.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5334 - accuracy: 0.7410\n",
      "Epoch 116/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7415\n",
      "Epoch 117/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5337 - accuracy: 0.7412\n",
      "Epoch 118/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7411\n",
      "Epoch 119/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5599 - accuracy: 0.7406\n",
      "Epoch 120/200\n",
      "782/804 [============================>.] - ETA: 0s - loss: 0.5332 - accuracy: 0.7414\n",
      "Epoch 120: saving model to optimization_checkpoints\\attempt_2_weights.120.hdf5\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5336 - accuracy: 0.7413\n",
      "Epoch 121/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7418\n",
      "Epoch 122/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5330 - accuracy: 0.7417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5340 - accuracy: 0.7404\n",
      "Epoch 124/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5334 - accuracy: 0.7410\n",
      "Epoch 125/200\n",
      "783/804 [============================>.] - ETA: 0s - loss: 0.5341 - accuracy: 0.7408\n",
      "Epoch 125: saving model to optimization_checkpoints\\attempt_2_weights.125.hdf5\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5333 - accuracy: 0.7414\n",
      "Epoch 126/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5333 - accuracy: 0.7415\n",
      "Epoch 127/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7407\n",
      "Epoch 128/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5331 - accuracy: 0.7401\n",
      "Epoch 129/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5337 - accuracy: 0.7407\n",
      "Epoch 130/200\n",
      "793/804 [============================>.] - ETA: 0s - loss: 0.5346 - accuracy: 0.7417\n",
      "Epoch 130: saving model to optimization_checkpoints\\attempt_2_weights.130.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7418\n",
      "Epoch 131/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7418\n",
      "Epoch 132/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5328 - accuracy: 0.7420\n",
      "Epoch 133/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5327 - accuracy: 0.7412\n",
      "Epoch 134/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5337 - accuracy: 0.7414\n",
      "Epoch 135/200\n",
      "782/804 [============================>.] - ETA: 0s - loss: 0.5335 - accuracy: 0.7408\n",
      "Epoch 135: saving model to optimization_checkpoints\\attempt_2_weights.135.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5339 - accuracy: 0.7405\n",
      "Epoch 136/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5337 - accuracy: 0.7415\n",
      "Epoch 137/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7413\n",
      "Epoch 138/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5335 - accuracy: 0.7416\n",
      "Epoch 139/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5331 - accuracy: 0.7414\n",
      "Epoch 140/200\n",
      "803/804 [============================>.] - ETA: 0s - loss: 0.5326 - accuracy: 0.7422\n",
      "Epoch 140: saving model to optimization_checkpoints\\attempt_2_weights.140.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5327 - accuracy: 0.7421\n",
      "Epoch 141/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5327 - accuracy: 0.7418\n",
      "Epoch 142/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5330 - accuracy: 0.7412\n",
      "Epoch 143/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5326 - accuracy: 0.7421\n",
      "Epoch 144/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5329 - accuracy: 0.7415\n",
      "Epoch 145/200\n",
      "797/804 [============================>.] - ETA: 0s - loss: 0.5887 - accuracy: 0.7398\n",
      "Epoch 145: saving model to optimization_checkpoints\\attempt_2_weights.145.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5879 - accuracy: 0.7398\n",
      "Epoch 146/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5327 - accuracy: 0.7421\n",
      "Epoch 147/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5323 - accuracy: 0.7419\n",
      "Epoch 148/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7422\n",
      "Epoch 149/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5326 - accuracy: 0.7419\n",
      "Epoch 150/200\n",
      "788/804 [============================>.] - ETA: 0s - loss: 0.5319 - accuracy: 0.7422\n",
      "Epoch 150: saving model to optimization_checkpoints\\attempt_2_weights.150.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5321 - accuracy: 0.7420\n",
      "Epoch 151/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5327 - accuracy: 0.7418\n",
      "Epoch 152/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5330 - accuracy: 0.7419\n",
      "Epoch 153/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5324 - accuracy: 0.7411\n",
      "Epoch 154/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7411\n",
      "Epoch 155/200\n",
      "799/804 [============================>.] - ETA: 0s - loss: 0.5342 - accuracy: 0.7409\n",
      "Epoch 155: saving model to optimization_checkpoints\\attempt_2_weights.155.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7412\n",
      "Epoch 156/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5328 - accuracy: 0.7415\n",
      "Epoch 157/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5324 - accuracy: 0.7421\n",
      "Epoch 158/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5325 - accuracy: 0.7416\n",
      "Epoch 159/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5358 - accuracy: 0.7408\n",
      "Epoch 160/200\n",
      "798/804 [============================>.] - ETA: 0s - loss: 0.5332 - accuracy: 0.7411\n",
      "Epoch 160: saving model to optimization_checkpoints\\attempt_2_weights.160.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5335 - accuracy: 0.7408\n",
      "Epoch 161/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5327 - accuracy: 0.7418\n",
      "Epoch 162/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5323 - accuracy: 0.7420\n",
      "Epoch 163/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5333 - accuracy: 0.7417\n",
      "Epoch 164/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5326 - accuracy: 0.7414\n",
      "Epoch 165/200\n",
      "798/804 [============================>.] - ETA: 0s - loss: 0.5336 - accuracy: 0.7417\n",
      "Epoch 165: saving model to optimization_checkpoints\\attempt_2_weights.165.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5336 - accuracy: 0.7417\n",
      "Epoch 166/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5332 - accuracy: 0.7416\n",
      "Epoch 167/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5332 - accuracy: 0.7420\n",
      "Epoch 168/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5334 - accuracy: 0.7417\n",
      "Epoch 169/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7420\n",
      "Epoch 170/200\n",
      "795/804 [============================>.] - ETA: 0s - loss: 0.5376 - accuracy: 0.7419\n",
      "Epoch 170: saving model to optimization_checkpoints\\attempt_2_weights.170.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5379 - accuracy: 0.7417\n",
      "Epoch 171/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5322 - accuracy: 0.7420\n",
      "Epoch 172/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5325 - accuracy: 0.7419\n",
      "Epoch 173/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5340 - accuracy: 0.7418\n",
      "Epoch 174/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5327 - accuracy: 0.7418\n",
      "Epoch 175/200\n",
      "792/804 [============================>.] - ETA: 0s - loss: 0.5332 - accuracy: 0.7414\n",
      "Epoch 175: saving model to optimization_checkpoints\\attempt_2_weights.175.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5330 - accuracy: 0.7414\n",
      "Epoch 176/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5323 - accuracy: 0.7415\n",
      "Epoch 177/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5325 - accuracy: 0.7420\n",
      "Epoch 178/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5361 - accuracy: 0.7406\n",
      "Epoch 179/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5332 - accuracy: 0.7416\n",
      "Epoch 180/200\n",
      "802/804 [============================>.] - ETA: 0s - loss: 0.5326 - accuracy: 0.7417\n",
      "Epoch 180: saving model to optimization_checkpoints\\attempt_2_weights.180.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5325 - accuracy: 0.7418\n",
      "Epoch 181/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5331 - accuracy: 0.7419\n",
      "Epoch 182/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5326 - accuracy: 0.7420\n",
      "Epoch 183/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5328 - accuracy: 0.7413\n",
      "Epoch 184/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5324 - accuracy: 0.7421\n",
      "Epoch 185/200\n",
      "788/804 [============================>.] - ETA: 0s - loss: 0.5326 - accuracy: 0.7416\n",
      "Epoch 185: saving model to optimization_checkpoints\\attempt_2_weights.185.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5323 - accuracy: 0.7420\n",
      "Epoch 186/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5330 - accuracy: 0.7394\n",
      "Epoch 187/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7411\n",
      "Epoch 188/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5325 - accuracy: 0.7414\n",
      "Epoch 189/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5325 - accuracy: 0.7420\n",
      "Epoch 190/200\n",
      "777/804 [===========================>..] - ETA: 0s - loss: 0.5339 - accuracy: 0.7420\n",
      "Epoch 190: saving model to optimization_checkpoints\\attempt_2_weights.190.hdf5\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5340 - accuracy: 0.7415\n",
      "Epoch 191/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7416\n",
      "Epoch 192/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5328 - accuracy: 0.7414\n",
      "Epoch 193/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7402\n",
      "Epoch 194/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5321 - accuracy: 0.7417\n",
      "Epoch 195/200\n",
      "778/804 [============================>.] - ETA: 0s - loss: 0.5325 - accuracy: 0.7424\n",
      "Epoch 195: saving model to optimization_checkpoints\\attempt_2_weights.195.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5323 - accuracy: 0.7421\n",
      "Epoch 196/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5327 - accuracy: 0.7409\n",
      "Epoch 197/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5333 - accuracy: 0.7410\n",
      "Epoch 198/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5333 - accuracy: 0.7418\n",
      "Epoch 199/200\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5328 - accuracy: 0.7416\n",
      "Epoch 200/200\n",
      "798/804 [============================>.] - ETA: 0s - loss: 0.5333 - accuracy: 0.7410\n",
      "Epoch 200: saving model to optimization_checkpoints\\attempt_2_weights.200.hdf5\n",
      "804/804 [==============================] - 1s 2ms/step - loss: 0.5335 - accuracy: 0.7409\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=200,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 - 0s - loss: 0.6159 - accuracy: 0.7278 - 420ms/epoch - 2ms/step\n",
      "Loss: 0.615918755531311, Accuracy: 0.7278134226799011\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "nn.save(\"AlphabetSoupCharity_Optimization_attempt_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 3: Increasing the hidden layers and changing the activation function to \"tanh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_11 (Dense)            (None, 160)               6880      \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 120)               19320     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 80)                9680      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 40)                3240      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 20)                820       \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 10)                210       \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,161\n",
      "Trainable params: 40,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 160\n",
    "hidden_nodes_layer2 = 120\n",
    "hidden_nodes_layer3 = 80\n",
    "hidden_nodes_layer4 = 40\n",
    "hidden_nodes_layer5 = 20\n",
    "hidden_nodes_layer6 = 10\n",
    "output_features = len(application_df[[\"IS_SUCCESSFUL\"]].columns)\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"tanh\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"tanh\"))\n",
    "\n",
    "# Third hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"tanh\"))\n",
    "\n",
    "# Fourth hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"tanh\"))\n",
    "\n",
    "# Fifth hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer5, activation=\"tanh\"))\n",
    "\n",
    "# Sixth hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer6, activation=\"tanh\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=output_features, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import checkpoint dependencies\n",
    "import os\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"optimization_checkpoints/\",exist_ok=True)\n",
    "checkpoint_path = \"optimization_checkpoints/attempt_3_weights.{epoch:03d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "# Create a callback that saves the model's weights every 5 epochs\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq = 'epoch',\n",
    "    period=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "804/804 [==============================] - 3s 2ms/step - loss: 0.5674 - accuracy: 0.7249\n",
      "Epoch 2/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5564 - accuracy: 0.7296\n",
      "Epoch 3/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5528 - accuracy: 0.7310\n",
      "Epoch 4/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5513 - accuracy: 0.7305\n",
      "Epoch 5/200\n",
      "804/804 [==============================] - ETA: 0s - loss: 0.5497 - accuracy: 0.7319\n",
      "Epoch 5: saving model to optimization_checkpoints\\attempt_3_weights.005.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7319\n",
      "Epoch 6/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5489 - accuracy: 0.7314\n",
      "Epoch 7/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5485 - accuracy: 0.7339\n",
      "Epoch 8/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5466 - accuracy: 0.7341\n",
      "Epoch 9/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5471 - accuracy: 0.7335\n",
      "Epoch 10/200\n",
      "802/804 [============================>.] - ETA: 0s - loss: 0.5476 - accuracy: 0.7338\n",
      "Epoch 10: saving model to optimization_checkpoints\\attempt_3_weights.010.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5477 - accuracy: 0.7338\n",
      "Epoch 11/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5461 - accuracy: 0.7347\n",
      "Epoch 12/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5449 - accuracy: 0.7361\n",
      "Epoch 13/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5440 - accuracy: 0.7362\n",
      "Epoch 14/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5447 - accuracy: 0.7346\n",
      "Epoch 15/200\n",
      "787/804 [============================>.] - ETA: 0s - loss: 0.5451 - accuracy: 0.7354\n",
      "Epoch 15: saving model to optimization_checkpoints\\attempt_3_weights.015.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5451 - accuracy: 0.7356\n",
      "Epoch 16/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5433 - accuracy: 0.7365\n",
      "Epoch 17/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5428 - accuracy: 0.7363\n",
      "Epoch 18/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5422 - accuracy: 0.7375\n",
      "Epoch 19/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5411 - accuracy: 0.7373\n",
      "Epoch 20/200\n",
      "803/804 [============================>.] - ETA: 0s - loss: 0.5412 - accuracy: 0.7370\n",
      "Epoch 20: saving model to optimization_checkpoints\\attempt_3_weights.020.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5412 - accuracy: 0.7371\n",
      "Epoch 21/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5420 - accuracy: 0.7367\n",
      "Epoch 22/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5407 - accuracy: 0.7366\n",
      "Epoch 23/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5399 - accuracy: 0.7383\n",
      "Epoch 24/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5396 - accuracy: 0.7377\n",
      "Epoch 25/200\n",
      "794/804 [============================>.] - ETA: 0s - loss: 0.5401 - accuracy: 0.7376\n",
      "Epoch 25: saving model to optimization_checkpoints\\attempt_3_weights.025.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.7379\n",
      "Epoch 26/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5394 - accuracy: 0.7388\n",
      "Epoch 27/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7388\n",
      "Epoch 28/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5391 - accuracy: 0.7391\n",
      "Epoch 29/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5402 - accuracy: 0.7378\n",
      "Epoch 30/200\n",
      "779/804 [============================>.] - ETA: 0s - loss: 0.5390 - accuracy: 0.7385\n",
      "Epoch 30: saving model to optimization_checkpoints\\attempt_3_weights.030.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5386 - accuracy: 0.7385\n",
      "Epoch 31/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5384 - accuracy: 0.7377\n",
      "Epoch 32/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5380 - accuracy: 0.7397\n",
      "Epoch 33/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5376 - accuracy: 0.7387\n",
      "Epoch 34/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5373 - accuracy: 0.7388\n",
      "Epoch 35/200\n",
      "791/804 [============================>.] - ETA: 0s - loss: 0.5385 - accuracy: 0.7392\n",
      "Epoch 35: saving model to optimization_checkpoints\\attempt_3_weights.035.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5390 - accuracy: 0.7385\n",
      "Epoch 36/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5384 - accuracy: 0.7381\n",
      "Epoch 37/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5378 - accuracy: 0.7381\n",
      "Epoch 38/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5373 - accuracy: 0.7392\n",
      "Epoch 39/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5371 - accuracy: 0.7385\n",
      "Epoch 40/200\n",
      "801/804 [============================>.] - ETA: 0s - loss: 0.5377 - accuracy: 0.7385\n",
      "Epoch 40: saving model to optimization_checkpoints\\attempt_3_weights.040.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5377 - accuracy: 0.7386\n",
      "Epoch 41/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5375 - accuracy: 0.7390\n",
      "Epoch 42/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5375 - accuracy: 0.7385\n",
      "Epoch 43/200\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5369 - accuracy: 0.7391\n",
      "Epoch 44/200\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5367 - accuracy: 0.7399\n",
      "Epoch 45/200\n",
      "787/804 [============================>.] - ETA: 0s - loss: 0.5373 - accuracy: 0.7400\n",
      "Epoch 45: saving model to optimization_checkpoints\\attempt_3_weights.045.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5375 - accuracy: 0.7402\n",
      "Epoch 46/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5368 - accuracy: 0.7397\n",
      "Epoch 47/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5359 - accuracy: 0.7397\n",
      "Epoch 48/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5363 - accuracy: 0.7402\n",
      "Epoch 49/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5366 - accuracy: 0.7404\n",
      "Epoch 50/200\n",
      "797/804 [============================>.] - ETA: 0s - loss: 0.5362 - accuracy: 0.7399\n",
      "Epoch 50: saving model to optimization_checkpoints\\attempt_3_weights.050.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5364 - accuracy: 0.7396\n",
      "Epoch 51/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5364 - accuracy: 0.7392\n",
      "Epoch 52/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5375 - accuracy: 0.7383\n",
      "Epoch 53/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5361 - accuracy: 0.7402\n",
      "Epoch 54/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7398\n",
      "Epoch 55/200\n",
      "794/804 [============================>.] - ETA: 0s - loss: 0.5354 - accuracy: 0.7403\n",
      "Epoch 55: saving model to optimization_checkpoints\\attempt_3_weights.055.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5352 - accuracy: 0.7410\n",
      "Epoch 56/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7406\n",
      "Epoch 57/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5361 - accuracy: 0.7399\n",
      "Epoch 58/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5351 - accuracy: 0.7405\n",
      "Epoch 59/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5359 - accuracy: 0.7394\n",
      "Epoch 60/200\n",
      "779/804 [============================>.] - ETA: 0s - loss: 0.5354 - accuracy: 0.7407\n",
      "Epoch 60: saving model to optimization_checkpoints\\attempt_3_weights.060.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5359 - accuracy: 0.7400\n",
      "Epoch 61/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5359 - accuracy: 0.7400\n",
      "Epoch 62/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5350 - accuracy: 0.7404\n",
      "Epoch 63/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5352 - accuracy: 0.7396\n",
      "Epoch 64/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5358 - accuracy: 0.7399\n",
      "Epoch 65/200\n",
      "785/804 [============================>.] - ETA: 0s - loss: 0.5360 - accuracy: 0.7398\n",
      "Epoch 65: saving model to optimization_checkpoints\\attempt_3_weights.065.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5359 - accuracy: 0.7400\n",
      "Epoch 66/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7408\n",
      "Epoch 67/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5352 - accuracy: 0.7405\n",
      "Epoch 68/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7407\n",
      "Epoch 69/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5353 - accuracy: 0.7411\n",
      "Epoch 70/200\n",
      "783/804 [============================>.] - ETA: 0s - loss: 0.5362 - accuracy: 0.7408\n",
      "Epoch 70: saving model to optimization_checkpoints\\attempt_3_weights.070.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7409\n",
      "Epoch 71/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5350 - accuracy: 0.7398\n",
      "Epoch 72/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5347 - accuracy: 0.7407\n",
      "Epoch 73/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5349 - accuracy: 0.7407\n",
      "Epoch 74/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5353 - accuracy: 0.7405\n",
      "Epoch 75/200\n",
      "798/804 [============================>.] - ETA: 0s - loss: 0.5349 - accuracy: 0.7407\n",
      "Epoch 75: saving model to optimization_checkpoints\\attempt_3_weights.075.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5353 - accuracy: 0.7404\n",
      "Epoch 76/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7403\n",
      "Epoch 77/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5347 - accuracy: 0.7399\n",
      "Epoch 78/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5347 - accuracy: 0.7414\n",
      "Epoch 79/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5354 - accuracy: 0.7406\n",
      "Epoch 80/200\n",
      "804/804 [==============================] - ETA: 0s - loss: 0.5345 - accuracy: 0.7406\n",
      "Epoch 80: saving model to optimization_checkpoints\\attempt_3_weights.080.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7406\n",
      "Epoch 81/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7407\n",
      "Epoch 82/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5347 - accuracy: 0.7414\n",
      "Epoch 83/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5354 - accuracy: 0.7417\n",
      "Epoch 84/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7392\n",
      "Epoch 85/200\n",
      "790/804 [============================>.] - ETA: 0s - loss: 0.5352 - accuracy: 0.7402\n",
      "Epoch 85: saving model to optimization_checkpoints\\attempt_3_weights.085.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5356 - accuracy: 0.7403\n",
      "Epoch 86/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5351 - accuracy: 0.7416\n",
      "Epoch 87/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7403\n",
      "Epoch 88/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5347 - accuracy: 0.7414\n",
      "Epoch 89/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5349 - accuracy: 0.7405\n",
      "Epoch 90/200\n",
      "794/804 [============================>.] - ETA: 0s - loss: 0.5365 - accuracy: 0.7391\n",
      "Epoch 90: saving model to optimization_checkpoints\\attempt_3_weights.090.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5364 - accuracy: 0.7393\n",
      "Epoch 91/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7404\n",
      "Epoch 92/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5339 - accuracy: 0.7413\n",
      "Epoch 93/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7403\n",
      "Epoch 94/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7406\n",
      "Epoch 95/200\n",
      "793/804 [============================>.] - ETA: 0s - loss: 0.5344 - accuracy: 0.7416\n",
      "Epoch 95: saving model to optimization_checkpoints\\attempt_3_weights.095.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7415\n",
      "Epoch 96/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7410\n",
      "Epoch 97/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7410\n",
      "Epoch 98/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7411\n",
      "Epoch 99/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7409\n",
      "Epoch 100/200\n",
      "786/804 [============================>.] - ETA: 0s - loss: 0.5349 - accuracy: 0.7399\n",
      "Epoch 100: saving model to optimization_checkpoints\\attempt_3_weights.100.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7403\n",
      "Epoch 101/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7410\n",
      "Epoch 102/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7413\n",
      "Epoch 103/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7418\n",
      "Epoch 104/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7403\n",
      "Epoch 105/200\n",
      "795/804 [============================>.] - ETA: 0s - loss: 0.5350 - accuracy: 0.7412\n",
      "Epoch 105: saving model to optimization_checkpoints\\attempt_3_weights.105.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5350 - accuracy: 0.7411\n",
      "Epoch 106/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7406\n",
      "Epoch 107/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7405\n",
      "Epoch 108/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7405\n",
      "Epoch 109/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7406\n",
      "Epoch 110/200\n",
      "804/804 [==============================] - ETA: 0s - loss: 0.5337 - accuracy: 0.7408\n",
      "Epoch 110: saving model to optimization_checkpoints\\attempt_3_weights.110.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5337 - accuracy: 0.7408\n",
      "Epoch 111/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5339 - accuracy: 0.7397\n",
      "Epoch 112/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7411\n",
      "Epoch 113/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5351 - accuracy: 0.7394\n",
      "Epoch 114/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7404\n",
      "Epoch 115/200\n",
      "799/804 [============================>.] - ETA: 0s - loss: 0.5338 - accuracy: 0.7406\n",
      "Epoch 115: saving model to optimization_checkpoints\\attempt_3_weights.115.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7404\n",
      "Epoch 116/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7408\n",
      "Epoch 117/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7404\n",
      "Epoch 118/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5349 - accuracy: 0.7412\n",
      "Epoch 119/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5349 - accuracy: 0.7404\n",
      "Epoch 120/200\n",
      "790/804 [============================>.] - ETA: 0s - loss: 0.5338 - accuracy: 0.7420\n",
      "Epoch 120: saving model to optimization_checkpoints\\attempt_3_weights.120.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5340 - accuracy: 0.7419\n",
      "Epoch 121/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5348 - accuracy: 0.7405\n",
      "Epoch 122/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5349 - accuracy: 0.7408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5355 - accuracy: 0.7405\n",
      "Epoch 124/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5349 - accuracy: 0.7405\n",
      "Epoch 125/200\n",
      "801/804 [============================>.] - ETA: 0s - loss: 0.5350 - accuracy: 0.7408\n",
      "Epoch 125: saving model to optimization_checkpoints\\attempt_3_weights.125.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5350 - accuracy: 0.7408\n",
      "Epoch 126/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5347 - accuracy: 0.7415\n",
      "Epoch 127/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5340 - accuracy: 0.7408\n",
      "Epoch 128/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7410\n",
      "Epoch 129/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7409\n",
      "Epoch 130/200\n",
      "804/804 [==============================] - ETA: 0s - loss: 0.5342 - accuracy: 0.7407\n",
      "Epoch 130: saving model to optimization_checkpoints\\attempt_3_weights.130.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7407\n",
      "Epoch 131/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5352 - accuracy: 0.7413\n",
      "Epoch 132/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7413\n",
      "Epoch 133/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7411\n",
      "Epoch 134/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5337 - accuracy: 0.7407\n",
      "Epoch 135/200\n",
      "782/804 [============================>.] - ETA: 0s - loss: 0.5342 - accuracy: 0.7422\n",
      "Epoch 135: saving model to optimization_checkpoints\\attempt_3_weights.135.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7419\n",
      "Epoch 136/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7408\n",
      "Epoch 137/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5349 - accuracy: 0.7407\n",
      "Epoch 138/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5348 - accuracy: 0.7408\n",
      "Epoch 139/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7408\n",
      "Epoch 140/200\n",
      "804/804 [==============================] - ETA: 0s - loss: 0.5342 - accuracy: 0.7407\n",
      "Epoch 140: saving model to optimization_checkpoints\\attempt_3_weights.140.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7407\n",
      "Epoch 141/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7407\n",
      "Epoch 142/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7405\n",
      "Epoch 143/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7407\n",
      "Epoch 144/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7405\n",
      "Epoch 145/200\n",
      "797/804 [============================>.] - ETA: 0s - loss: 0.5332 - accuracy: 0.7415\n",
      "Epoch 145: saving model to optimization_checkpoints\\attempt_3_weights.145.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5335 - accuracy: 0.7413\n",
      "Epoch 146/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5337 - accuracy: 0.7407\n",
      "Epoch 147/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7410\n",
      "Epoch 148/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5339 - accuracy: 0.7413\n",
      "Epoch 149/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7414\n",
      "Epoch 150/200\n",
      "786/804 [============================>.] - ETA: 0s - loss: 0.5344 - accuracy: 0.7407\n",
      "Epoch 150: saving model to optimization_checkpoints\\attempt_3_weights.150.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7408\n",
      "Epoch 151/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7403\n",
      "Epoch 152/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5348 - accuracy: 0.7406\n",
      "Epoch 153/200\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5342 - accuracy: 0.7409\n",
      "Epoch 154/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5349 - accuracy: 0.7405\n",
      "Epoch 155/200\n",
      "785/804 [============================>.] - ETA: 0s - loss: 0.5342 - accuracy: 0.7404\n",
      "Epoch 155: saving model to optimization_checkpoints\\attempt_3_weights.155.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7404\n",
      "Epoch 156/200\n",
      "804/804 [==============================] - 2s 3ms/step - loss: 0.5336 - accuracy: 0.7405\n",
      "Epoch 157/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7407\n",
      "Epoch 158/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7402\n",
      "Epoch 159/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7406\n",
      "Epoch 160/200\n",
      "783/804 [============================>.] - ETA: 0s - loss: 0.5338 - accuracy: 0.7408\n",
      "Epoch 160: saving model to optimization_checkpoints\\attempt_3_weights.160.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7407\n",
      "Epoch 161/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5337 - accuracy: 0.7409\n",
      "Epoch 162/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5352 - accuracy: 0.7407\n",
      "Epoch 163/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7409\n",
      "Epoch 164/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7409\n",
      "Epoch 165/200\n",
      "800/804 [============================>.] - ETA: 0s - loss: 0.5346 - accuracy: 0.7410\n",
      "Epoch 165: saving model to optimization_checkpoints\\attempt_3_weights.165.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7410\n",
      "Epoch 166/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7407\n",
      "Epoch 167/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5351 - accuracy: 0.7401\n",
      "Epoch 168/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7404\n",
      "Epoch 169/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7410\n",
      "Epoch 170/200\n",
      "800/804 [============================>.] - ETA: 0s - loss: 0.5345 - accuracy: 0.7407\n",
      "Epoch 170: saving model to optimization_checkpoints\\attempt_3_weights.170.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7409\n",
      "Epoch 171/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7407\n",
      "Epoch 172/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7402\n",
      "Epoch 173/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7408\n",
      "Epoch 174/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5347 - accuracy: 0.7401\n",
      "Epoch 175/200\n",
      "795/804 [============================>.] - ETA: 0s - loss: 0.5341 - accuracy: 0.7406\n",
      "Epoch 175: saving model to optimization_checkpoints\\attempt_3_weights.175.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7404\n",
      "Epoch 176/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7406\n",
      "Epoch 177/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7409\n",
      "Epoch 178/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5347 - accuracy: 0.7402\n",
      "Epoch 179/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7405\n",
      "Epoch 180/200\n",
      "782/804 [============================>.] - ETA: 0s - loss: 0.5348 - accuracy: 0.7405\n",
      "Epoch 180: saving model to optimization_checkpoints\\attempt_3_weights.180.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7410\n",
      "Epoch 181/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7404\n",
      "Epoch 182/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7412\n",
      "Epoch 183/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7403\n",
      "Epoch 184/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7406\n",
      "Epoch 185/200\n",
      "785/804 [============================>.] - ETA: 0s - loss: 0.5339 - accuracy: 0.7409\n",
      "Epoch 185: saving model to optimization_checkpoints\\attempt_3_weights.185.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5350 - accuracy: 0.7397\n",
      "Epoch 186/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5348 - accuracy: 0.7402\n",
      "Epoch 187/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7405\n",
      "Epoch 188/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5339 - accuracy: 0.7399\n",
      "Epoch 189/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7407\n",
      "Epoch 190/200\n",
      "799/804 [============================>.] - ETA: 0s - loss: 0.5344 - accuracy: 0.7394\n",
      "Epoch 190: saving model to optimization_checkpoints\\attempt_3_weights.190.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7396\n",
      "Epoch 191/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5353 - accuracy: 0.7403\n",
      "Epoch 192/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5348 - accuracy: 0.7401\n",
      "Epoch 193/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7402\n",
      "Epoch 194/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7400\n",
      "Epoch 195/200\n",
      "802/804 [============================>.] - ETA: 0s - loss: 0.5342 - accuracy: 0.7409\n",
      "Epoch 195: saving model to optimization_checkpoints\\attempt_3_weights.195.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7407\n",
      "Epoch 196/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7407\n",
      "Epoch 197/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5340 - accuracy: 0.7406\n",
      "Epoch 198/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7400\n",
      "Epoch 199/200\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7411\n",
      "Epoch 200/200\n",
      "802/804 [============================>.] - ETA: 0s - loss: 0.5349 - accuracy: 0.7407\n",
      "Epoch 200: saving model to optimization_checkpoints\\attempt_3_weights.200.hdf5\n",
      "804/804 [==============================] - 2s 2ms/step - loss: 0.5347 - accuracy: 0.7408\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled,y_train,epochs=200,callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 - 1s - loss: 0.5575 - accuracy: 0.7259 - 552ms/epoch - 2ms/step\n",
      "Loss: 0.5575281381607056, Accuracy: 0.7259474992752075\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "nn.save(\"AlphabetSoupCharity_Optimization_attempt_3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 4: Try to achieve our goal of 75% accuracy using \"keras-tuner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method that creates a new Sequential model with hyperparameter options\n",
    "def create_model(hp):\n",
    "    nn_model = tf.keras.models.Sequential()\n",
    "\n",
    "    # Allow kerastuner to decide which activation function to use in hidden layers\n",
    "    activation = hp.Choice('activation',['relu','tanh','selu'])\n",
    "    \n",
    "    # Allow kerastuner to decide number of neurons in first layer\n",
    "    number_of_nerons = hp.Int('first_units', min_value=1, max_value=200, step=5)\n",
    "    nn_model.add(tf.keras.layers.Dense(units=number_of_nerons, activation=activation, input_dim=number_input_features))\n",
    "\n",
    "    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers\n",
    "    for i in range(hp.Int('num_layers', 1, 8)):\n",
    "        number_of_nerons = hp.Int('units_' + str(i), min_value=1, max_value=200, step=5)\n",
    "        nn_model.add(tf.keras.layers.Dense(units=number_of_nerons, activation=activation))\n",
    "    \n",
    "    nn_model.add(tf.keras.layers.Dense(units=output_features, activation=\"sigmoid\"))\n",
    "\n",
    "    # Compile the model\n",
    "    nn_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
    "    \n",
    "    return nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the kerastuner library\n",
    "import keras_tuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    create_model,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_epochs=20,\n",
    "    hyperband_iterations=2,\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 60 Complete [00h 00m 47s]\n",
      "val_accuracy: 0.7281632423400879\n",
      "\n",
      "Best val_accuracy So Far: 0.7293294668197632\n",
      "Total elapsed time: 00h 18m 39s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Run the kerastuner search for best hyperparameters\n",
    "tuner.search(X_train_scaled,y_train,epochs=20,validation_data=(X_test_scaled,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'tanh',\n",
       " 'first_units': 131,\n",
       " 'num_layers': 3,\n",
       " 'units_0': 71,\n",
       " 'units_1': 21,\n",
       " 'units_2': 191,\n",
       " 'units_3': 131,\n",
       " 'units_4': 136,\n",
       " 'units_5': 76,\n",
       " 'units_6': 131,\n",
       " 'units_7': 156,\n",
       " 'tuner/epochs': 20,\n",
       " 'tuner/initial_epoch': 7,\n",
       " 'tuner/bracket': 2,\n",
       " 'tuner/round': 2,\n",
       " 'tuner/trial_id': '0014'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get best model hyperparameters\n",
    "best_hyper = tuner.get_best_hyperparameters(1)[0]\n",
    "best_hyper.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 - 1s - loss: 0.5524 - accuracy: 0.7293 - 594ms/epoch - 2ms/step\n",
      "Loss: 0.5523509383201599, Accuracy: 0.7293294668197632\n"
     ]
    }
   ],
   "source": [
    "# Evaluate best model against full test data\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "model_loss, model_accuracy = best_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export our model to HDF5 file\n",
    "nn.save(\"AlphabetSoupCharity_Optimization_attempt_4_with_keras_tuner.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
